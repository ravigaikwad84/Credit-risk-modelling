{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stocks Sentiment Analysis Using AI - Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravigaikwad84/Credit-risk-modelling/blob/main/Stocks_Sentiment_Analysis_Using_AI_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LUX0LaYmKNa"
      },
      "source": [
        "#TASK #1: UNDESTAND THE PROBLEM STATEMENT AND BUSINESS CASE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm43sLloJnSQ"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1djxup79_KiGtKFiH7AgSD0Bj-2D90TBg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n-3qhTpXjsy"
      },
      "source": [
        "#TASK #2: IMPORT LIBRARIES/DATASETS AND PERFORM EXPLORATORY DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpiddPjsl_4Q"
      },
      "source": [
        "# import key libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "import plotly.express as px\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWWbtOTf4kS-",
        "outputId": "c7a1acc8-df5d-4c29-e0a9-02c32388559c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmtxigpoQZh4",
        "outputId": "e114bdd0-2330-4762-ab36-709e3e567f49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# install nltk\n",
        "# NLTK: Natural Language tool kit\n",
        "!pip install nltk"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaWz3q0OQg6h",
        "outputId": "73b0720c-0671-4da9-d6c5-944c69d895fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# install gensim\n",
        "# Gensim is an open-source library for unsupervised topic modeling and natural language processing\n",
        "# Gensim is implemented in Python and Cython.\n",
        "!pip install gensim"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/drive/MyDrive/Classroom/9 AM BATCH/NLP"
      ],
      "metadata": {
        "id": "1lBfcJgpt15K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QiTczEunJNx"
      },
      "source": [
        "# load the stock news data\n",
        "stock_df = pd.read_csv('/content/drive/MyDrive/Classroom/9 AM BATCH/stock_sentiment.csv')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB2gQ1w4nR-P",
        "outputId": "e67512a2-78de-4ed1-f062-a3143182e626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "# Let's view the dataset \n",
        "stock_df"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   Text  Sentiment\n",
              "0     Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1\n",
              "1     user: AAP MOVIE. 55% return for the FEA/GEED i...          1\n",
              "2     user I'd be afraid to short AMZN - they are lo...          1\n",
              "3                                     MNTA Over 12.00            1\n",
              "4                                      OI  Over 21.37            1\n",
              "...                                                 ...        ...\n",
              "5786  Industry body CII said #discoms are likely to ...          0\n",
              "5787  #Gold prices slip below Rs 46,000 as #investor...          0\n",
              "5788  Workers at Bajaj Auto have agreed to a 10% wag...          1\n",
              "5789  #Sharemarket LIVE: Sensex off dayâ€™s high, up 6...          1\n",
              "5790  #Sensex, #Nifty climb off day's highs, still u...          1\n",
              "\n",
              "[5791 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-54b1bc38-718f-44b7-9608-ae9d36d29133\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MNTA Over 12.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>OI  Over 21.37</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5786</th>\n",
              "      <td>Industry body CII said #discoms are likely to ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5787</th>\n",
              "      <td>#Gold prices slip below Rs 46,000 as #investor...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5788</th>\n",
              "      <td>Workers at Bajaj Auto have agreed to a 10% wag...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5789</th>\n",
              "      <td>#Sharemarket LIVE: Sensex off dayâ€™s high, up 6...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5790</th>\n",
              "      <td>#Sensex, #Nifty climb off day's highs, still u...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5791 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54b1bc38-718f-44b7-9608-ae9d36d29133')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-54b1bc38-718f-44b7-9608-ae9d36d29133 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-54b1bc38-718f-44b7-9608-ae9d36d29133');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEVxeFq5Zlsd"
      },
      "source": [
        "# dataframe information\n",
        "stock_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH5W6t7fZoQS"
      },
      "source": [
        "# check for null values\n",
        "stock_df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-sKr-33Y8YM"
      },
      "source": [
        "**MINI CHALLENGE #1:**\n",
        "\n",
        "- **How many unique elements are present in the 'sentiment' column? Try to find out with two different methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjtge71bZxhw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgkZHO3CPnsp"
      },
      "source": [
        "#TASK #3: PERFORM DATA CLEANING (REMOVE PUNCTUATIONS FROM TEXT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAWaIZEd6cYj"
      },
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbWfzII36cWc"
      },
      "source": [
        "Test = '$I love AI & Machine learning!!'\n",
        "Test_punc_removed = [char for char in Test if char not in string.punctuation]\n",
        "Test_punc_removed_join = ''.join(Test_punc_removed)\n",
        "Test_punc_removed_join"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvhT_EjX6cUQ"
      },
      "source": [
        "Test = 'Good morning beautiful people :)... #I am having fun learning Finance with Python!!'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM76PxJ36cSQ"
      },
      "source": [
        "Test_punc_removed = [char for char in Test if char not in string.punctuation]\n",
        "Test_punc_removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmG7ijks6aqt"
      },
      "source": [
        "# Join the characters again to form the string.\n",
        "Test_punc_removed_join = ''.join(Test_punc_removed)\n",
        "Test_punc_removed_join"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE4dRfZG_O8O"
      },
      "source": [
        "# Let's define a function to remove punctuations\n",
        "def remove_punc(message):\n",
        "    Test_punc_removed = [char for char in message if char not in string.punctuation]\n",
        "    Test_punc_removed_join = ''.join(Test_punc_removed)\n",
        "\n",
        "    return Test_punc_removed_join"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KW-IgnK-dV4"
      },
      "source": [
        "# Let's remove punctuations from our dataset \n",
        "stock_df['Text Without Punctuation'] = stock_df['Text'].apply(remove_punc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU1Ubd45_3t-"
      },
      "source": [
        "stock_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GklP2yj5Ab_D"
      },
      "source": [
        "stock_df['Text'][2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohgg1iNHAe4W"
      },
      "source": [
        "stock_df['Text Without Punctuation'][2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnSDZVMH7Joa"
      },
      "source": [
        "**MINI CHALLENGE #2:** \n",
        "- **Remove punctuations using a different method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRjfAhXL68XV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w1Rm4Zm68Zi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2fMUtl66ztE"
      },
      "source": [
        "# TASK #4: PERFORM DATA CLEANING (REMOVE STOPWORDS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmEkZj3k69S7"
      },
      "source": [
        "# download stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyDMuSWA69Y2"
      },
      "source": [
        "# Obtain additional stopwords from nltk\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use','will','aap','co','day','user','stock','today','week','year'])\n",
        "# stop_words.extend(['from', 'subject', 're', 'edu', 'use','will','aap','co','day','user','stock','today','week','year', 'https'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy1n42a069Vr"
      },
      "source": [
        "# Remove stopwords and remove short words (less than 2 characters)\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if len(token) >= 3 and token not in stop_words:\n",
        "            result.append(token)\n",
        "            \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv1Igmf87a5I"
      },
      "source": [
        "# apply pre-processing to the text column\n",
        "stock_df['Text Without Punc & Stopwords'] = stock_df['Text Without Punctuation'].apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCgsA4vG9kxj"
      },
      "source": [
        "stock_df['Text'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-WDumvb93wC"
      },
      "source": [
        "stock_df['Text Without Punc & Stopwords'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bYql7en7hcs"
      },
      "source": [
        "# join the words into a string\n",
        "# stock_df['Processed Text 2'] = stock_df['Processed Text 2'].apply(lambda x: \" \".join(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9JqZBzf9CEY"
      },
      "source": [
        "stock_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA4VMxIf7mha"
      },
      "source": [
        "**MINI CHALLENGE #3:**\n",
        "\n",
        "- **Modify the code in order keep words that are longer than or equal 2 characters instead of 3**\n",
        "- **Add 'https' to the list of stopwords and rerun the code**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3arHVxiNCFIA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RUFuufl-Hx8"
      },
      "source": [
        "# TASK #5: PLOT WORDCLOUD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJAYGiUwg-Ez"
      },
      "source": [
        "# join the words into a string\n",
        "stock_df['Text Without Punc & Stopwords Joined'] = stock_df['Text Without Punc & Stopwords'].apply(lambda x: \" \".join(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ErWl2jf7hYS"
      },
      "source": [
        "# plot the word cloud for text with positive sentiment\n",
        "plt.figure(figsize = (20, 20)) \n",
        "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800).generate(\" \".join(stock_df[stock_df['Sentiment'] == 1]['Text Without Punc & Stopwords Joined']))\n",
        "plt.imshow(wc, interpolation = 'bilinear');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vt7d0CqVO4H"
      },
      "source": [
        "**MINI CHALLENGE #4:**\n",
        "- **Visualize the wordcloud for tweets that have negative sentiment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2m28Qrtds_n"
      },
      "source": [
        "# TASK #6: VISUALIZE CLEANED DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB8NRW0fIQRj"
      },
      "source": [
        "stock_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u93O9tHHiRfd"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgVLvPBLiZGb"
      },
      "source": [
        "# word_tokenize is used to break up a string into words\n",
        "print(stock_df['Text Without Punc & Stopwords Joined'][0])\n",
        "print(nltk.word_tokenize(stock_df['Text Without Punc & Stopwords Joined'][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r20ny06ECP1B"
      },
      "source": [
        "# Obtain the maximum length of data in the document\n",
        "# This will be later used when word embeddings are generated\n",
        "maxlen = -1\n",
        "for doc in stock_df['Text Without Punc & Stopwords Joined']:\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    if(maxlen < len(tokens)):\n",
        "        maxlen = len(tokens)\n",
        "print(\"The maximum number of words in any document is:\", maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffCdD2BZjFyP"
      },
      "source": [
        "tweets_length = [ len(nltk.word_tokenize(x)) for x in stock_df['Text Without Punc & Stopwords Joined'] ]\n",
        "tweets_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDtUZMRVcD7I"
      },
      "source": [
        "# Plot the distribution for the number of words in a text\n",
        "fig = px.histogram(x = tweets_length, nbins = 50)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPTNSE6_Xd-b"
      },
      "source": [
        "**MINI CHALLENGE #5:**\n",
        "- **Use Seaborn Countplot to visually indicate how many samples belong to the positive and negative sentiments class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaj3nkiaYHLX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi3neznbP_QT"
      },
      "source": [
        "# TASK #7: PREPARE THE DATA BY TOKENIZING AND PADDING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0MfgDJ8K-Jm"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=13j8m-JOpK994CtukR1EShiY_hGGjkNx-)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZHGI_qYj6Iw"
      },
      "source": [
        "stock_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alPKDD0P7a28"
      },
      "source": [
        "# Obtain the total words present in the dataset\n",
        "list_of_words = []\n",
        "for i in stock_df['Text Without Punc & Stopwords']:\n",
        "    for j in i:\n",
        "        list_of_words.append(j)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN75gigvYVxc"
      },
      "source": [
        "list_of_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN5g6jTF7a1I"
      },
      "source": [
        "# Obtain the total number of unique words\n",
        "total_words = len(list(set(list_of_words)))\n",
        "total_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUgEf-SZ7R7c"
      },
      "source": [
        "# split the data into test and train \n",
        "X = stock_df['Text Without Punc & Stopwords']\n",
        "y = stock_df['Sentiment']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgFE9ss1JmCw"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeoabpYjnS3o"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcyTkiaZlQCK"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kme-IYsM6uJa"
      },
      "source": [
        "# Create a tokenizer to tokenize the words and create sequences of tokenized words\n",
        "tokenizer = Tokenizer(num_words = total_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Training data\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "# Testing data\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tGqwPkoiXea"
      },
      "source": [
        "train_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3joXOZeieD0"
      },
      "source": [
        "test_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEWCpgXYNF1r"
      },
      "source": [
        "print(\"The encoding for document\\n\", X_train[1:2],\"\\n is: \", train_sequences[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URy4Wkai_Qh3"
      },
      "source": [
        "# Add padding to training and testing\n",
        "padded_train = pad_sequences(train_sequences, maxlen = 29, padding = 'post', truncating = 'post')\n",
        "padded_test = pad_sequences(test_sequences, maxlen = 29, truncating = 'post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D60mkoZvSG5D"
      },
      "source": [
        "for i, doc in enumerate(padded_train[:3]):\n",
        "     print(\"The padded encoding for document:\", i+1,\" is:\", doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlUjvz5UPZm0"
      },
      "source": [
        "# Convert the data to categorical 2D representation\n",
        "y_train_cat = to_categorical(y_train, 2)\n",
        "y_test_cat = to_categorical(y_test, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqPAOMmfPhxm"
      },
      "source": [
        "y_train_cat.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MYO-dWSjIHq"
      },
      "source": [
        "y_test_cat.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "und7m_TDjGf3"
      },
      "source": [
        "y_train_cat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL_1IF0xo2x6"
      },
      "source": [
        "**MINI CHALLENGE #6:**\n",
        "\n",
        "- **Change the padding length to 15 and rerun the code. Verify that padding was successful**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLpwzbPwo2b9"
      },
      "source": [
        "# Add padding to training and testing\n",
        "padded_train = pad_sequences(train_sequences, maxlen = 15, padding = 'post', truncating = 'post')\n",
        "padded_test = pad_sequences(test_sequences, maxlen = 15, truncating = 'post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOufWm8yjaoW"
      },
      "source": [
        "# TASK #8: UNDERSTAND THE THEORY AND INTUITION BEHIND RECURRENT NEURAL NETWORKS AND LONG SHORT TERM MEMORY NETWORKS (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVsilx4zPUIj"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1Giaz7q1THBFTuNFpSyLBKnoUbbvWlNw3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KOnwBjvPdFj"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1iDKpQqmGTNr3riuQOvXdiwfy9wlCU5st)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU_qrPe5Pmkd"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1PxW6DBer4d1Q9_9OSaAQDTtqUdDGLdYa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-9Q8dOojRIf"
      },
      "source": [
        "# TASK #9: BUILD A CUSTOM-BASED DEEP NEURAL NETWORK TO PERFORM SENTIMENT ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMzQsewkLKAn"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1zpI1XHM1CSxLPjsW7QTahfs_f2stzKeQ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k_ZJfGLjQhc"
      },
      "source": [
        "# Sequential Model\n",
        "model = Sequential()\n",
        "\n",
        "# embedding layer\n",
        "model.add(Embedding(total_words, output_dim = 512))\n",
        "\n",
        "# Bi-Directional RNN and LSTM\n",
        "model.add(LSTM(256))\n",
        "\n",
        "# Dense layers\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(2,activation = 'softmax'))\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhbQTkvfjNVi"
      },
      "source": [
        "# train the model\n",
        "model.fit(padded_train, y_train_cat, batch_size = 32, validation_split = 0.2, epochs = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teWbFh2eqbIA"
      },
      "source": [
        "**MINI CHALLENGE #7:**\n",
        "- **Train the model using different embedding output dimension**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kc3H3LRjrIp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9UIM9ugjrb0"
      },
      "source": [
        "# TASK #10: ASSESS TRAINED MODEL PERFORMANCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAZtLrGTPvlx"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1MZdb0g69XDC4JRATR9K6-2NAkrclGAXO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qRUkys-BSuQ"
      },
      "source": [
        "# make prediction\n",
        "pred = model.predict(padded_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh66RfZgF7ln"
      },
      "source": [
        "# make prediction\n",
        "prediction = []\n",
        "for i in pred:\n",
        "  prediction.append(np.argmax(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z3ka_OTQK6G"
      },
      "source": [
        "# list containing original values\n",
        "original = []\n",
        "for i in y_test_cat:\n",
        "  original.append(np.argmax(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxjH9_yZQj_m"
      },
      "source": [
        "# acuracy score on text data\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(original, prediction)\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rpgzbSqHfR4"
      },
      "source": [
        "# Plot the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(original, prediction)\n",
        "sns.heatmap(cm, annot = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYY2JDUVuL64"
      },
      "source": [
        "**MINI CHALLENGE #8:**\n",
        "\n",
        "- **Use a pretrained BERT model to make a sentiment analysis predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhwl15DAucKD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54LObFe0Y-vB"
      },
      "source": [
        "# **EXCELLENT JOB!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNuSau6LZC9I"
      },
      "source": [
        "# **MINI CHALLENGE SOLUTIONS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IRjyZV-Yhdl"
      },
      "source": [
        "**MINI CHALLENGE #1 SOLUTION:**\n",
        "\n",
        "- **How many unique elements are present in the 'sentiment' column? Try to find out with two different methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imlxP0Q9YcR3"
      },
      "source": [
        "sns.countplot(stock_df['Sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX8eMi_v47u5"
      },
      "source": [
        "# Find the number of unique values in a particular column\n",
        "stock_df['Sentiment'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lid7lewH7GYZ"
      },
      "source": [
        "**MINI CHALLENGE #2 SOLUTION:** \n",
        "- **Remove punctuations using a different method**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfkdX7N-6bHD"
      },
      "source": [
        "Test_punc_removed = []\n",
        "for char in Test: \n",
        "    if char not in string.punctuation:\n",
        "        Test_punc_removed.append(char)\n",
        "\n",
        "# Join the characters again to form the string.\n",
        "Test_punc_removed_join = ''.join(Test_punc_removed)\n",
        "Test_punc_removed_join"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qBbEYEtTadZ"
      },
      "source": [
        "**MINI CHALLENGE #3 SOLUTION:**\n",
        "\n",
        "- **Modify the code in order keep words that are longer than or equal 2 characters instead of 3**\n",
        "- **Add 'https' to the list of stopwords and rerun the code**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MwBYHnhTZYR"
      },
      "source": [
        "# Obtain additional stopwords from nltk\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use','will','aap','co','day','user','stock','today','week','year', 'https'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIxqGCFvTlQC"
      },
      "source": [
        "# Remove stopwords and remove words with 2 or less characters\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >= 2 and token not in stop_words:\n",
        "            result.append(token)\n",
        "            \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMqaQhvXVxDm"
      },
      "source": [
        "**MINI CHALLENGE #4 SOLTUTION:**\n",
        "- **Visualize the wordcloud for tweets that have negative sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZoc-8qR7ayv"
      },
      "source": [
        "# plot the word cloud for text that is negative\n",
        "plt.figure(figsize = (20,20)) \n",
        "wc = WordCloud(max_words = 1000, width = 1600, height = 800 ).generate(\" \".join(stock_df[stock_df['Sentiment'] == 0]['Text Without Punc & Stopwords Joined']))\n",
        "plt.imshow(wc, interpolation = 'bilinear');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FRzUbePXB4q"
      },
      "source": [
        "**MINI CHALLENGE #5 SOLUTION:**\n",
        "- **Use Seaborn Countplot to visually indicate how many samples belong to the positive and negative sentiments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoRESyH1dxiw"
      },
      "source": [
        "# plot the word count\n",
        "sns.countplot(stock_df['Sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VegekE5ppO4A"
      },
      "source": [
        "**MINI CHALLENGE #6 SOLUTION:**\n",
        "\n",
        "- **Change the padding length to 15 and rerun the code. Verify that padding was successful**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkxyi3jtpO4E"
      },
      "source": [
        "# Add padding to training and testing\n",
        "padded_train = pad_sequences(train_sequences, maxlen = 15, padding = 'post', truncating = 'post')\n",
        "padded_test = pad_sequences(test_sequences, maxlen = 15, truncating = 'post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCrSJiesquw1"
      },
      "source": [
        "**MINI CHALLENGE #7 SOLUTION:**\n",
        "- **Train the model using different embedding output dimension**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzEEVItSquw_"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# embedding layer\n",
        "model.add(Embedding(total_words, output_dim = 256))\n",
        "\n",
        "# Bi-Directional RNN and LSTM\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "\n",
        "# Dense layers\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dense(1,activation = 'sigmoid'))\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZAKgzEgufIL"
      },
      "source": [
        "**MINI CHALLENGE #8 SOLTUTION:**\n",
        "\n",
        "- **Use a pretrained BERT model to make a sentiment analysis predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU_SxUJQu_iA"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJrZVwVcXivM"
      },
      "source": [
        "# Use pipeline from transformer to perform specific task. \n",
        "# Mention sentiment analysis as task and pass in the string to it, to get the results\n",
        "# We can specify tasks like topic modeling, Q and A, text summarization here.\n",
        "from transformers import pipeline\n",
        "\n",
        "nlp = pipeline('sentiment-analysis')\n",
        "\n",
        "# Make prediction on the test data\n",
        "pred = nlp(list(X_test))\n",
        "\n",
        "# Since predicted value is a dictionary, get the label from the dict\n",
        "prediction = []\n",
        "for i in pred:\n",
        "  prediction.append(i['label'])\n",
        "\n",
        "# print the final results\n",
        "for i in range(len(prediction[:3])):\n",
        "  print(\"\\n\\nNews :\\n\\n\", df[df.combined == X_test.values[i]].Text.item(), \"\\n\\nOriginal value :\\n\\n\",\n",
        "      category[df[df.combined == X_test.values[i]].Sentiment.item()], \"\\n\\nPredicted value :\\n\\n\", prediction[i], \"\\n\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NRMvwi5vCgR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}